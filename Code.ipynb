{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a448829e",
   "metadata": {},
   "source": [
    "Group Menbers:\n",
    "- Didier Bakoue Ngatcha \n",
    "- Paul Laudrup Fotso Kaptue\n",
    "- Wilfried Djomeni Djiela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90339d4c",
   "metadata": {},
   "source": [
    "# Data Challenge: Prediction of authors' h-index\n",
    "\n",
    "## Description\n",
    "\n",
    "### Description of the subject\n",
    "\n",
    "The goal of this challenge is to study and apply machine learning / artificial intelligence methods to a real-world regression problem. In this problem, each data corresponds to an author and we are asked to predict the h-index of this author. The h-index of an author measures its productivity and its impact in the research field. It is defined as the maximum value h such that the author has published h paper(s) that have each been cited at least h times. To build the model, we have:\n",
    "- a graph that shapes the intensity of collaboration between researchers\n",
    "- extracts from authors' papers\n",
    "\n",
    "\n",
    "### Description of the data \n",
    "we have the following files:\n",
    "- coauthorship.edges : it is a graph where nodes correspond to authors and edges specify whether or not two authors have collaborated together for the production of a research paper. This graph contains 217801 vertices (authors) and 1718164 edges.\n",
    "- author_papers.txt: contains a list of authors and IDs of their most cited papers\n",
    "- abstract.txt: for each paper, this file contains the Id of the paper and the \"inverted Index\" of the extracts of this paper\n",
    "- train.csv: contains 174242 authors and their h-index. Each line contains the author ID and its h-index.\n",
    "- test.csv : contains 43561 author IDs whose h-indexes we want to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66438a1e",
   "metadata": {},
   "source": [
    "### Simple Regression Lasso\n",
    "this first submission produced an MSE of 129.0, so there is a question of improving it as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a5a3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# read training data\n",
    "df_train = pd.read_csv('train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges()\n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)\n",
    "\n",
    "\n",
    "# computes structural features for each node\n",
    "core_number = nx.core_number(G)\n",
    "\n",
    "# create the training matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number \n",
    "X_train = np.zeros((n_train, 2))\n",
    "y_train = np.zeros(n_train)\n",
    "nodes_train = np.zeros((n_train, 1))\n",
    "for i,row in df_train.iterrows():\n",
    "    node = row['author']\n",
    "    X_train[i,0] = G.degree(node)\n",
    "    X_train[i,1] = core_number[node]\n",
    "    y_train[i] = row['hindex']\n",
    "    nodes_train[i, 0] = node\n",
    "    \n",
    "\n",
    "# create the test matrix. each node is represented as a vector of 3 features:\n",
    "# (1) its degree, (2) its core number\n",
    "X_test = np.zeros((n_test, 2))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = row['author']\n",
    "    X_test[i,0] = G.degree(node)\n",
    "    X_test[i,1] = core_number[node]\n",
    "    \n",
    "# train a regression model and make predictions\n",
    "reg = Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# write the predictions to file\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "\n",
    "\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd04a19",
   "metadata": {},
   "source": [
    "## Features extraction\n",
    "Let's recall that we have the extracts of the authors' papers and the collaboration graph.\n",
    "The goal of this first part is to extract from these two sets of vectors that will characterize the authors.\n",
    "### abstract features extraction\n",
    "#### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939ed30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n",
      "C:\\Users\\Wilfried\\anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "Skipping line 63243: Expected 2 fields in line 63243, saw 19. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 82978: Expected 2 fields in line 82978, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 123264: Expected 2 fields in line 123264, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 150525: Expected 2 fields in line 150525, saw 4. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 198059: Expected 2 fields in line 198059, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 297016: Expected 2 fields in line 297016, saw 4. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 368499: Expected 2 fields in line 368499, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 454769: Expected 2 fields in line 454769, saw 22. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 465821: Expected 2 fields in line 465821, saw 38. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 551720: Expected 2 fields in line 551720, saw 19. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 614820: Expected 2 fields in line 614820, saw 16. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 623076: Expected 2 fields in line 623076, saw 11. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
      "Skipping line 623828: Expected 2 fields in line 623828, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n"
     ]
    }
   ],
   "source": [
    "#loading abstract and author papers in dataframe\n",
    "abstract_dict = pd.read_csv('abstracts.txt', sep=\"----\", \n",
    "                            error_bad_lines=False, \n",
    "                            header=None)\n",
    "author_paper = pd.read_csv('author_papers.txt', sep=':' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ea4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string \n",
    "\n",
    "# put the abstrack in a form that can be use for training \n",
    "# the result text_abstract is use for the training of tfidf\n",
    "# the result train_abstract is use to train our word to vec model \n",
    "def build_text_abstract(dict_abstract:dict):\n",
    "    words_matrix = [\"\" for i  in range(dict_abstract['IndexLength'])]\n",
    "    for key in list(dict_abstract['InvertedIndex'].keys()):\n",
    "        for position in dict_abstract['InvertedIndex'][key]:\n",
    "            words_matrix[position] = key\n",
    "    text_abstract = ''.join(word+' ' for word in words_matrix)\n",
    "    for punc in string.punctuation:\n",
    "        text_abstract = text_abstract.replace(punc, ' ')\n",
    "    for num in range (10):\n",
    "        text_abstract = text_abstract.replace(str(num), '')\n",
    "    text_abstract = text_abstract.lower()\n",
    "    train_abstract = text_abstract.split()\n",
    "    return str(text_abstract) , train_abstract\n",
    "\n",
    "# put the result in two generator , one for tfidf and the oder for word2vec training \n",
    "abstracts = (build_text_abstract(json.loads(abstract_dict[1][i]))[0] for i in range (len(abstract_dict)))\n",
    "trainings_abstracts = (build_text_abstract(json.loads(abstract_dict[1][i]))[1] for i in range (len(abstract_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85bfb3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624168"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(trainings_abstracts)\n",
    "# len(list(abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff6eba",
   "metadata": {},
   "source": [
    "#### strategy: TFIDF weighted word2vec \n",
    "To build information representing authors according to their research paper excerpt, we chose to use word2vec which we will train on the data. We will then build vectors representing the authors' abstracts by modulating the vectors produced by word2vec with the TFIDF of the different words in the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd6c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilfried\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#build the tfidf for all the abstracts \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_abs = vectorizer.fit_transform(abs for abs in abstracts)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "vectors_gen = (tfidf_abs[i] for i in range (tfidf_abs.get_shape()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb333b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427456"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7424fa",
   "metadata": {},
   "source": [
    "**The goal of the following cell is to train our own word2vec neural network. this training took us a lot of time(about 7hours). There is no need to recompute it. our model has already been save and the cell after this one upload it** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training of a word2vec there is no need to train the model again. it has already be trained and the has only to be loaded \n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "#     callback to print the loss after each epoch\n",
    "    def __init__ (self):\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        model.save('abstractsw3vecIt.model')\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        elif self.epoch % 1 == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "\n",
    "\n",
    "models_abstracts = Word2Vec(list(trainings_abstracts), \n",
    "                            vector_size=300, \n",
    "                            min_count=1, \n",
    "                            workers=4, \n",
    "                            window=5)\n",
    "\n",
    "models_abstracts.build_vocab(sentences)\n",
    "\n",
    "# train the word2vec neurals network models on my dataset abstracts \n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "models_abstracts.train(sentences, \n",
    "                      total_examples=models_abstracts.corpus_count,\n",
    "                      epochs=50,\n",
    "                      report_delay=1,\n",
    "                      compute_loss=True,\n",
    "                      callbacks=[callback()])\n",
    "end = time.time()\n",
    "\n",
    "models_abstracts.save('abstractsw2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b1a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 489681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('companies', 0.6483214497566223),\n",
       " ('customers', 0.6270943880081177),\n",
       " ('customer', 0.601658046245575),\n",
       " ('taikang', 0.5920459628105164),\n",
       " ('company’s', 0.5890681743621826),\n",
       " ('utopics', 0.5839164853096008),\n",
       " ('organizations', 0.5789316892623901),\n",
       " ('employees', 0.5730001926422119),\n",
       " ('financial', 0.5720323324203491),\n",
       " ('bangchak', 0.5690685510635376)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the pretrained word2vec model \n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "reload_model = Word2Vec.load('abstractsw2vecIt.model')\n",
    "# models_abstracts.wv.vocab\n",
    "words = list(reload_model.wv.index_to_key)\n",
    "print('vocabulary size :', len(words))\n",
    "\n",
    "reload_model.wv.most_similar(positive='company', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1a18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute now the list of abstract vect\n",
    "reload_model.wv.add_vectors(feature_name, np.zeros((len(feature_name) ,300)), replace=False)\n",
    "vocab_vects = reload_model.wv[feature_name]\n",
    "\n",
    "# weighted word2vec with tfidf\n",
    "abs_vects_list = tfidf_abs@vocab_vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ddb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute vectors that represent each authors\n",
    "vects_authors = []\n",
    "nbre_total = 0\n",
    "for index , papers in enumerate (author_paper['paperID']):\n",
    "    list_papers = papers.split('-')\n",
    "    vec_author = np.zeros(300)\n",
    "    nb_paper = 0\n",
    "    nbre_total += len(list_papers)   \n",
    "    for paper in list_papers:\n",
    "        if len(np.array(df['abs_vects'][df.Id == int(paper)])) != 0:\n",
    "            vec_author += np.array(df['abs_vects'][df.Id == int(paper)])[0]\n",
    "            nb_paper += 1\n",
    "#     print(vec_author) \n",
    "#     print(nb_paper)\n",
    "    if nb_paper != 0:\n",
    "        vec_author /= nb_paper\n",
    "    vects_authors.append(vec_author)\n",
    "print(nbre_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baad4cd",
   "metadata": {},
   "source": [
    "### Graph features\n",
    "\n",
    "we use to strategy to compute graph feature :\n",
    "- the graph metric : there are feature compute by *hand* and that represent for each node a property it has \n",
    "- Node2Vec : node to is an embedding method for node's graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute graph metrics : there is no need to compute graph features again. they have already been computed and stored in the csv with abstract vect\n",
    "\n",
    "def compute_features(g, node):\n",
    "    X = np.zeros((1, 3))\n",
    "\n",
    "    neighb = [n for n in g.neighbors(node)]\n",
    "    nb_neighb = len(neighb)\n",
    "    neighb.append(node)\n",
    "    g1 = g.subgraph(neighb)\n",
    "    neighb = neighb[:-1]\n",
    "    #groups\n",
    "    t = set(neighb)\n",
    "    res = []\n",
    "    while len(t)!=0:\n",
    "        prev_len=0\n",
    "        clus = set({neighb[0]})\n",
    "        while prev_len!=len(clus):\n",
    "            prev_len = len(clus)\n",
    "            temp = {c for c in clus}\n",
    "            for c in clus:\n",
    "                temp.update(g1.neighbors(c))\n",
    "                temp.remove(node)\n",
    "            clus = {m for m in temp}\n",
    "        res.append(clus)\n",
    "        for k in clus:\n",
    "            neighb.remove(k)\n",
    "        t = set(neighb)\n",
    "    #corresponding h-index\n",
    "    #features\n",
    "\n",
    "    nb_comp =0\n",
    "    nb_isolates=0\n",
    "    nb_auth_in_comp = []\n",
    "#     h_idx_per_comp = []\n",
    "#     h_idx_per_isolates = []\n",
    "    for k in range(len(res)):\n",
    "        if len(res[k])>=2:\n",
    "            nb_auth_in_comp.append(len(res[k]))\n",
    "            nb_comp = nb_comp+1\n",
    "            \n",
    "        elif len(res[k])==1:\n",
    "            nb_isolates = nb_isolates+1\n",
    "            \n",
    "    X[0, 0] = nb_neighb\n",
    "    X[0, 1] = nb_comp\n",
    "    X[0, 2] = nb_isolates \n",
    "    return X\n",
    "\n",
    "def neighbor_av_degree(g, node):\n",
    "    \n",
    "    d = 0\n",
    "    for neighb in g.neighbors(node):\n",
    "        d+=g.degree[neighb]\n",
    "    if g.degree[node] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return d/g.degree[node]\n",
    "\n",
    "dictionary_d_centrality =nx.algorithms.centrality.degree_centrality(G)\n",
    "def degree_centrality(node):\n",
    "    return dictionary_d_centrality[node]\n",
    "\n",
    "dictionary_of_page_rank = nx.pagerank(G)\n",
    "def page_rank(node):\n",
    "    return dictionary_of_page_rank[node]\n",
    "\n",
    "dictionary_of_core = nx.algorithms.core.core_number(G)\n",
    "def core_number(node):\n",
    "    return dictionary_of_core[node]\n",
    "\n",
    "def neighbor_av_and_max_h(g, node, author_labelled):\n",
    "    h_indices = []\n",
    "    for neighb in g.neighbors(node):\n",
    "#         k = np.argwhere(nodes_train[:,0]==neighb).flatten()\n",
    "        if not np.isnan(author_labelled['hindex'][neighb]):\n",
    "            h_indices.append(author_labelled['hindex'][neighb])\n",
    "    if len(h_indices)!=0:\n",
    "        return np.mean(h_indices), np.max(h_indices)\n",
    "    else:\n",
    "        return 0, 0\n",
    "    \n",
    "dictionary_of_b_centrality = nx.betweenness_centrality(G)\n",
    "def b_centrality(node):\n",
    "    return dictionary_of_b_centrality[node]\n",
    "\n",
    "dictionary_of_EVC = nx.eigenvector_centrality(G)\n",
    "def h_of_mostEVC(g, node, author_labelled):\n",
    "    evc = []\n",
    "    evc_node = []\n",
    "    for neighb in g.neighbors(node):\n",
    "        if neighb in dictionary_of_EVC.keys():\n",
    "            evc.append(dictionary_of_EVC[neighb])\n",
    "            evc_node.append(neighb)\n",
    "    if len(evc)!=0:\n",
    "        max_node = evc_node[np.argmax(evc)]\n",
    "#         k = np.argwhere(nodes_train[:,0]==max_node).flatten()\n",
    "        if not np.isnan(author_labelled['hindex'][max_node]) : \n",
    "            return author_labelled['hindex'][max_node]\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "339f4dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node2Vec(1036333, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#node2vec embedding of nodes\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "\n",
    "modelvect = Node2Vec(edges.coalesce().indices(), embedding_dim=100, walk_length=3000,\n",
    "                     context_size=10, walks_per_node=1000,\n",
    "                     num_negative_samples=1, p=0.6, q=0.8, sparse=True).cuda()\n",
    "\n",
    "loader = modelvect.loader(shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.SparseAdam(list(modelvect.parameters()), lr=0.01)\n",
    "modelvect.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f1a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv \n",
    "\n",
    "# read the file that containt auhor labelled and graph features\n",
    "author_labelled = pd.read_csv('author_lbl_graphfeature2.csv')\n",
    "\n",
    "#modify nodes names and build edges indices for GCN computation\n",
    "mapping = {author_labelled['index'][i] : i for i in range(217800)}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "adjacency = nx.to_scipy_sparse_matrix(G)\n",
    "adjacency_coo = adjacency.tocoo()\n",
    "edges_0 = []\n",
    "edges_1 = []\n",
    "for edge in G.edges():\n",
    "    edges_0.append(edge[0])\n",
    "    edges_0.append(edge[1])\n",
    "    edges_1.append(edge[1])\n",
    "    edges_1.append(edge[0])\n",
    "\n",
    "adjacency_tensor = torch.sparse.LongTensor(torch.LongTensor([np.array(edges_0).tolist(),\n",
    "                                                             np.array(edges_1).tolist()]),\n",
    "                                                           torch.LongTensor(adjacency_coo.data.astype(np.int32)))\n",
    "\n",
    "edges = adjacency_tensor.cuda()\n",
    "\n",
    "\n",
    "\n",
    "X_train = author_labelled.drop(['index', 'hindex', 'paperID', 'mask', 'Unnamed: 0', 'out', 'Unnamed: 0.1'], axis=1)\n",
    "y_train = author_labelled['hindex']\n",
    "mask = author_labelled['mask'].astype('bool')\n",
    "\n",
    "X_train_np = pd.DataFrame(X_train).to_numpy()\n",
    "y_train_np = pd.DataFrame(y_train).to_numpy()\n",
    "mask_np = pd.DataFrame(mask).to_numpy()\n",
    "\n",
    "X_trp = X_train_np[mask_np.reshape(217800)]\n",
    "y_trp = y_train_np[mask_np.reshape(217800)]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_np)\n",
    "y_train_tensor = torch.tensor(y_train_np)\n",
    "mask_tensor = torch.tensor(mask.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2db2d9",
   "metadata": {},
   "source": [
    "**after all, this is the data frame witch contains all our data features**\n",
    "\n",
    "the hindex=nan for author of the test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f07b6ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>hindex</th>\n",
       "      <th>paperID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>page_ranks</th>\n",
       "      <th>degree_centralities</th>\n",
       "      <th>neighbor_av_degrees</th>\n",
       "      <th>nbr_connexions</th>\n",
       "      <th>nbr_isolates</th>\n",
       "      <th>nbr_comp</th>\n",
       "      <th>out</th>\n",
       "      <th>h_of_mostEVC</th>\n",
       "      <th>neighbor_av</th>\n",
       "      <th>neighbor_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1101850</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133459021-179719743-2111787673-2126488676-3183...</td>\n",
       "      <td>-0.516787</td>\n",
       "      <td>1.636447</td>\n",
       "      <td>-0.062156</td>\n",
       "      <td>-0.188258</td>\n",
       "      <td>-0.998227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.040722</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.635898</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.096257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1336878</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2122092249-2132109814-2100271871-2065672539-20...</td>\n",
       "      <td>-0.412267</td>\n",
       "      <td>1.622659</td>\n",
       "      <td>0.097246</td>\n",
       "      <td>-0.548176</td>\n",
       "      <td>-0.497817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>0.068780</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>0.068780</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>43.550365</td>\n",
       "      <td>0.252688</td>\n",
       "      <td>0.051974</td>\n",
       "      <td>0.390374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1515524</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2141827797-2127085795-2013547785-2138529788-19...</td>\n",
       "      <td>-0.324699</td>\n",
       "      <td>1.140772</td>\n",
       "      <td>0.147425</td>\n",
       "      <td>0.294866</td>\n",
       "      <td>-0.846317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>11.592803</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.165775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1606427</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1907724546</td>\n",
       "      <td>-0.228558</td>\n",
       "      <td>1.052226</td>\n",
       "      <td>0.286160</td>\n",
       "      <td>0.778895</td>\n",
       "      <td>-0.780682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.016043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2728936</td>\n",
       "      <td>4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2114261446-2042751882-1912205781-2059913822-19...</td>\n",
       "      <td>-0.246059</td>\n",
       "      <td>1.145682</td>\n",
       "      <td>0.077173</td>\n",
       "      <td>0.257870</td>\n",
       "      <td>-0.600490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071081</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.399586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064024</td>\n",
       "      <td>0.144385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217795</th>\n",
       "      <td>217795</td>\n",
       "      <td>2908277686</td>\n",
       "      <td>217795</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1964777539-2051142510-2092148526-2036760475-20...</td>\n",
       "      <td>-0.279820</td>\n",
       "      <td>1.364193</td>\n",
       "      <td>0.986596</td>\n",
       "      <td>-0.521121</td>\n",
       "      <td>-1.115616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.111261</td>\n",
       "      <td>0.331109</td>\n",
       "      <td>0.111261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>24.768595</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.353481</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217796</th>\n",
       "      <td>217796</td>\n",
       "      <td>2908387141</td>\n",
       "      <td>217796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2540479521</td>\n",
       "      <td>-0.165670</td>\n",
       "      <td>1.739194</td>\n",
       "      <td>0.239138</td>\n",
       "      <td>-0.301512</td>\n",
       "      <td>-0.595904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.162876</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.021390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217797</th>\n",
       "      <td>217797</td>\n",
       "      <td>2908425732</td>\n",
       "      <td>217797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2553344037</td>\n",
       "      <td>-0.608998</td>\n",
       "      <td>1.042386</td>\n",
       "      <td>-0.358419</td>\n",
       "      <td>-0.518114</td>\n",
       "      <td>-0.351695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.021649</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037634</td>\n",
       "      <td>0.079268</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217798</th>\n",
       "      <td>217798</td>\n",
       "      <td>2908436250</td>\n",
       "      <td>217798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2907086791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025378</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.034227</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.397518</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.219251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217799</th>\n",
       "      <td>217799</td>\n",
       "      <td>2908499439</td>\n",
       "      <td>217799</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2081432213-2070621672-2079679191-32110345-2013...</td>\n",
       "      <td>-0.302264</td>\n",
       "      <td>1.428584</td>\n",
       "      <td>0.197913</td>\n",
       "      <td>-0.220545</td>\n",
       "      <td>-0.818043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.549567</td>\n",
       "      <td>0.091398</td>\n",
       "      <td>0.103659</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217800 rows × 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       index  Unnamed: 0.1  hindex  \\\n",
       "0                0     1101850             0     NaN   \n",
       "1                1     1336878             1     NaN   \n",
       "2                2     1515524             2     7.0   \n",
       "3                3     1606427             3     1.0   \n",
       "4                4     2728936             4    27.0   \n",
       "...            ...         ...           ...     ...   \n",
       "217795      217795  2908277686        217795    28.0   \n",
       "217796      217796  2908387141        217796     NaN   \n",
       "217797      217797  2908425732        217797     1.0   \n",
       "217798      217798  2908436250        217798     1.0   \n",
       "217799      217799  2908499439        217799     5.0   \n",
       "\n",
       "                                                  paperID         0         1  \\\n",
       "0       133459021-179719743-2111787673-2126488676-3183... -0.516787  1.636447   \n",
       "1       2122092249-2132109814-2100271871-2065672539-20... -0.412267  1.622659   \n",
       "2       2141827797-2127085795-2013547785-2138529788-19... -0.324699  1.140772   \n",
       "3                                              1907724546 -0.228558  1.052226   \n",
       "4       2114261446-2042751882-1912205781-2059913822-19... -0.246059  1.145682   \n",
       "...                                                   ...       ...       ...   \n",
       "217795  1964777539-2051142510-2092148526-2036760475-20... -0.279820  1.364193   \n",
       "217796                                         2540479521 -0.165670  1.739194   \n",
       "217797                                         2553344037 -0.608998  1.042386   \n",
       "217798                                         2907086791  0.000000  0.000000   \n",
       "217799  2081432213-2070621672-2079679191-32110345-2013... -0.302264  1.428584   \n",
       "\n",
       "               2         3         4  ...  page_ranks  degree_centralities  \\\n",
       "0      -0.062156 -0.188258 -0.998227  ...    0.006489             0.001349   \n",
       "1       0.097246 -0.548176 -0.497817  ...    0.340766             0.068780   \n",
       "2       0.147425  0.294866 -0.846317  ...    0.015702             0.002023   \n",
       "3       0.286160  0.778895 -0.780682  ...    0.026008             0.001349   \n",
       "4       0.077173  0.257870 -0.600490  ...    0.071081             0.004046   \n",
       "...          ...       ...       ...  ...         ...                  ...   \n",
       "217795  0.986596 -0.521121 -1.115616  ...    0.025557             0.111261   \n",
       "217796  0.239138 -0.301512 -0.595904  ...    0.021564             0.001349   \n",
       "217797 -0.358419 -0.518114 -0.351695  ...    0.019159             0.002697   \n",
       "217798  0.000000  0.000000  0.000000  ...    0.025378             0.006743   \n",
       "217799  0.197913 -0.220545 -0.818043  ...    0.010311             0.000674   \n",
       "\n",
       "        neighbor_av_degrees  nbr_connexions  nbr_isolates  nbr_comp  \\\n",
       "0                  0.040722        0.001349      0.000000  0.000000   \n",
       "1                  0.020598        0.068780      0.277778  0.083333   \n",
       "2                  0.010653        0.002023      0.000000  0.083333   \n",
       "3                  0.002577        0.001349      0.000000  0.000000   \n",
       "4                  0.004639        0.004046      0.111111  0.000000   \n",
       "...                     ...             ...           ...       ...   \n",
       "217795             0.331109        0.111261      0.000000  0.083333   \n",
       "217796             0.003093        0.001349      0.000000  0.000000   \n",
       "217797             0.021649        0.002697      0.000000  0.083333   \n",
       "217798             0.034227        0.006743      0.000000  0.083333   \n",
       "217799             0.003093        0.000674      0.055556  0.000000   \n",
       "\n",
       "              out  h_of_mostEVC  neighbor_av  neighbor_max  \n",
       "0        3.635898      0.096774     0.073171      0.096257  \n",
       "1       43.550365      0.252688     0.051974      0.390374  \n",
       "2       11.592803      0.166667     0.113821      0.165775  \n",
       "3        0.000000      0.000000     0.018293      0.016043  \n",
       "4       24.399586      0.000000     0.064024      0.144385  \n",
       "...           ...           ...          ...           ...  \n",
       "217795  24.768595      0.419355     0.353481      0.529412  \n",
       "217796   2.162876      0.021505     0.018293      0.021390  \n",
       "217797   0.000000      0.037634     0.079268      0.090909  \n",
       "217798   1.397518      0.182796     0.105691      0.219251  \n",
       "217799  14.549567      0.091398     0.103659      0.090909  \n",
       "\n",
       "[217800 rows x 317 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_labelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97d7a3",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e3b17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(130, 70)\n",
      "  (out): Linear(in_features=70, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#build GCN structure\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv #GATConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        #Initialize the layers\n",
    "        self.conv1 = GCNConv(130, hidden_channels)\n",
    "        self.out = Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        #First Message passing Layer (Transformation)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x= F.dropout(x, p=0.1, training=self.training)\n",
    "        \n",
    "        #output layer\n",
    "        x= self.out(x)\n",
    "        return x\n",
    "model1 = GCN(hidden_channels=70)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ef272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model  GCN MODEL and send it to GPU for fast computation\n",
    "model1 = GCN(hidden_channels=70)\n",
    "# pd_to\n",
    "# Use GPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = model1.cuda()\n",
    "y_train_tensor = y_train_tensor.cuda()\n",
    "X_train_tensor = X_train_tensor.cuda()\n",
    "edges = adjacency_tensor.cuda()\n",
    "model1.to(torch.float64)\n",
    "#Initialize Optimizer \n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "optimizer = torch.optim.Adam(model1.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b50967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function (Mean scared error for regression  Problem )\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "def train():\n",
    "    model1.train()\n",
    "    optimizer.zero_grad()\n",
    "    #Use all data as input, because all nodes have node features\n",
    "    out = model1(X_train_tensor, edges.coalesce().indices())\n",
    "    #only use nodes with labels available for loss calculation --> mask\n",
    "#     print(out[mask_tensor].shape)\n",
    "    loss = criterion(out[mask_tensor], y_train_tensor.reshape((217800, 1))[mask_tensor])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval\n",
    "    out = model(data.x, data.edge_index)\n",
    "    #use the class with highest probability\n",
    "    pred = out.argmax(dim=1)\n",
    "    #check against ground-truth labels\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    #derive ratio of correct predictions\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "losses = []\n",
    "for epoch in range(0, 4000):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch: {epoch:03d}, Loss: {loss:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eebd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the prediction \n",
    "model.eval\n",
    "out = model(X_train_tensor, edges.coalesce().indices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04b1facf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 89.9711 - val_loss: 69.2476\n",
      "Epoch 2/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 73.6361 - val_loss: 64.9965\n",
      "Epoch 3/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 70.2883 - val_loss: 61.7423\n",
      "Epoch 4/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 67.9012 - val_loss: 60.1249\n",
      "Epoch 5/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 65.9814 - val_loss: 58.9922\n",
      "Epoch 6/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 64.5627 - val_loss: 63.3146\n",
      "Epoch 7/15\n",
      "4356/4356 [==============================] - 14s 3ms/step - loss: 63.4617 - val_loss: 58.0854\n",
      "Epoch 8/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 62.4959 - val_loss: 56.7446\n",
      "Epoch 9/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 61.4377 - val_loss: 57.1480\n",
      "Epoch 10/15\n",
      "4356/4356 [==============================] - 12s 3ms/step - loss: 60.7321 - val_loss: 55.4813\n",
      "Epoch 11/15\n",
      "4356/4356 [==============================] - 13s 3ms/step - loss: 60.2587 - val_loss: 62.2304\n",
      "Epoch 12/15\n",
      "4356/4356 [==============================] - 12s 3ms/step - loss: 59.5675 - val_loss: 55.5009\n",
      "Epoch 13/15\n",
      "4356/4356 [==============================] - 12s 3ms/step - loss: 59.0467 - val_loss: 56.2458\n",
      "Epoch 14/15\n",
      "4356/4356 [==============================] - 12s 3ms/step - loss: 58.6053 - val_loss: 53.8974\n",
      "Epoch 15/15\n",
      "4356/4356 [==============================] - 12s 3ms/step - loss: 57.9574 - val_loss: 56.3536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21699b85b20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "X_trp_val, X_test_val, y_trp_val, y_test_val = train_test_split(X_trp, y_trp, test_size=0.2, random_state=5)\n",
    "\n",
    "model1 = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=(310,), activation='relu', kernel_initializer = 'glorot_normal'),\n",
    "     keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(1, activation='relu'),  \n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "model1.compile(optimizer=opt,\n",
    "             loss='MeanSquaredError')\n",
    "model1.fit(X_trp_val, y_trp_val, epochs=15, validation_data=(X_test_val, y_test_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a45aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST Regressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "mse_max = -1\n",
    "optimal_n = -1\n",
    "for n_estimator in tqdm(range(10, 1000, 50)):\n",
    "    xgbr = XGBRegressor(n_estimators = n_estimator, verbosity=0)\n",
    "    xgbr.fit(X_trp_val, y_trp_val)\n",
    "\n",
    "    ypred_train = xgbr.predict(X_trp_val)\n",
    "    ypred_test = xgbr.predict(X_test_val)\n",
    "    mse_test_curr = mean_squared_error(y_test_val, ypred_test)\n",
    "    mse_train_curr = mean_squared_error(y_trp_val, ypred_train)\n",
    "    if mse_test_curr < mse_max:\n",
    "        optimal_n = n_estimator\n",
    "        mse_max = mse_test\n",
    "    mse_train.append(mse_train_curr)\n",
    "    mse_test.append(mse_test_curr)\n",
    "\n",
    "n_list = np.arange(10, 1000, 50)\n",
    "plt.plot(n_list, mse_train, label='Boost train')    \n",
    "plt.plot(n_list, mse_test, label='Boost test')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e37f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr.fit(X_trp_val, y_trp_val)\n",
    "score = xgbr.score(X_trp_val, y_trp_val)\n",
    "print('training score:', score)\n",
    "cv_score = cross_val_score(model1, X_trp_val, y_trp_val, cv=10)\n",
    "print(\"CV mean score :\", cv_score.mean)\n",
    "ypred = xgbr.predict(X_test_val)\n",
    "mse = mean_squared_error(y_test_val, ypred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", mse**(1/2.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
